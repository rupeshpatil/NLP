{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "\n",
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\"\n",
    "               \n",
    "# Tokenizing sentences\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "# Tokenizing words\n",
    "words = nltk.word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "\n",
    "Process of reducing infected words to their word Stem\n",
    "\n",
    "History      ---------      histori\n",
    "historical\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Finally\n",
    "Final        -----------          fina\n",
    "Finalized\n",
    "\n",
    "In the real world scenario if we want to sentiment analysis/moview review rating. we have to look for base word to understand actual sentiment of the word.\n",
    "Exa. - sentiment analysis, Spam\n",
    "\n",
    "Problem: Producing intermediate representation of the word may not have any meaning.\n",
    "\n",
    "example - fina,histori\n",
    "\n",
    "To overcome above problem we use lemmatization.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Stopwords \n",
    "\n",
    "Stop words are the words which is useful in english sentence to complete grammitical sentences. But not useful in sentiment analysis/spam detection so we have to remove such words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I three vision india .', 'In 3000 year histori , peopl world come invad us , captur land , conquer mind .', 'from alexand onward , greek , turk , mogul , portugues , british , french , dutch , came loot us , took .', 'yet done nation .', 'We conquer anyon .', 'We grab land , cultur , histori tri enforc way life .', 'whi ?', 'becaus respect freedom others.that first vision freedom .', 'I believ india got first vision 1857 , start war independ .', 'It freedom must protect nurtur build .', 'If free , one respect us .', 'My second vision india ’ develop .', 'for fifti year develop nation .', 'It time see develop nation .', 'We among top 5 nation world term gdp .', 'We 10 percent growth rate area .', 'our poverti level fall .', 'our achiev global recognis today .', 'yet lack self-confid see develop nation , self-reli self-assur .', 'isn ’ incorrect ?', 'I third vision .', 'india must stand world .', 'becaus I believ unless india stand world , one respect us .', 'onli strength respect strength .', 'We must strong militari power also econom power .', 'both must go hand-in-hand .', 'My good fortun work three great mind .', 'dr. vikram sarabhai dept .', 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .', 'I lucki work three close consid great opportun life .', 'I see four mileston career']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Tokenizing means convert into list of sentences from paragraph/string or\n",
    "# list of words from string/sentence\n",
    "# its\n",
    "# Tokenizing the paragraph into sentences.\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "# Download all english stop words\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# First remove stop words then applied steming\n",
    "for i in range(len(sentences)):\n",
    "    # tokenize sentence into words\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(w) for w in words if w not in stop_words]\n",
    "    sentences[i] = \" \".join(words)\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "\n",
    "Reducing word into meaningful word.\n",
    "\n",
    "\n",
    "History      ---------      history\n",
    "historical\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Finally\n",
    "Final        -----------          final\n",
    "Finalized\n",
    "\n",
    "Example - Chat bot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I three vision India .', 'In 3000 year history , people world come invaded u , captured land , conquered mind .', 'From Alexander onwards , Greeks , Turks , Moguls , Portuguese , British , French , Dutch , came looted u , took .', 'Yet done nation .', 'We conquered anyone .', 'We grabbed land , culture , history tried enforce way life .', 'Why ?', 'Because respect freedom others.That first vision freedom .', 'I believe India got first vision 1857 , started War Independence .', 'It freedom must protect nurture build .', 'If free , one respect u .', 'My second vision India ’ development .', 'For fifty year developing nation .', 'It time see developed nation .', 'We among top 5 nation world term GDP .', 'We 10 percent growth rate area .', 'Our poverty level falling .', 'Our achievement globally recognised today .', 'Yet lack self-confidence see developed nation , self-reliant self-assured .', 'Isn ’ incorrect ?', 'I third vision .', 'India must stand world .', 'Because I believe unless India stand world , one respect u .', 'Only strength respect strength .', 'We must strong military power also economic power .', 'Both must go hand-in-hand .', 'My good fortune worked three great mind .', 'Dr. Vikram Sarabhai Dept .', 'space , Professor Satish Dhawan , succeeded Dr. Brahm Prakash , father nuclear material .', 'I lucky worked three closely consider great opportunity life .', 'I see four milestone career']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "# Tokenizing the paragraph into sentences.\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "word_lem = WordNetLemmatizer()\n",
    "# First remove stop words then applied Lemmatization\n",
    "for i in range(len(sentences)):\n",
    "    # tokenize sentence into words\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [word_lem.lemmatize(w) for w in words if w not in stop_words]\n",
    "    sentences[i] = \" \".join(words)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Get the frequency of words into vector.\n",
    "There are two types of Bag words\n",
    " - Binary\n",
    " - Bag of words\n",
    "\n",
    "exa. - sent1 = \"He is intelligent boy\"\n",
    "       sent2 = \"she is intelligent girl\"\n",
    "       sent3 = \"boy and girl are intelligents\"\n",
    "Binary vector -----\n",
    "    Here we will go sentence by sentence and will \n",
    "    look for word and mark them as 1 if present and 0 for not present.\n",
    "                \n",
    "            boy   girl  intelligent\n",
    "        \n",
    "    sent1   1      0     1\n",
    "    sent2   0      1     1\n",
    "    sent3   1      1     1\n",
    "    \n",
    "BOW vector   ---------\n",
    "    words      frequency\n",
    "    boy          2\n",
    "    intelligent  3\n",
    "    girl         2\n",
    "\n",
    "In bag words are not able find correct weightage of words so alternatives are TF-IDF/Word2Vec\n",
    "\n",
    "Exam - we can use BOW for Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "word_lem = WordNetLemmatizer()\n",
    "# Cleanin the text below are the steps we have to do as per requirement\n",
    "# removinng the stop words, stemming/lemmatization\n",
    "# lower case words\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "   \n",
    "    review = re.sub('^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [stemmer.stem(w) for w in review if w not in stop_words]\n",
    "    review = \" \".join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "# creating the bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "cv = CountVectorizer(max_features =5000)\n",
    "x = cv.fit_transform(corpus)\n",
    "x = cv.fit_transform(corpus).toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TF\n",
    "No. of repeatead of words in sentence/no of words in sentence\n",
    "\n",
    "After tokenizing ,stopswords and lemanitaztion below are the sentence\n",
    "exa. - sent1 = \"intelligent boy\"\n",
    "       sent2 = \"intelligent girl\"\n",
    "       sent3 = \"boy girl intelligent\"\n",
    "\n",
    "                TF \n",
    "                sent1   sent2   sent3    \n",
    "    intelligent   1/2    1/2    1/3\n",
    "    girl           0     1/2    1/3\n",
    "    boy            1/2    0     1/3\n",
    "\n",
    "# IDF \n",
    "Log ( No of sentences / No. of sentences containing of words)\n",
    "\n",
    "words        IDF\n",
    "intelligent  log(3/3)\n",
    "boy          log(3/2)\n",
    "girl         log (3/2)\n",
    "\n",
    "finally \n",
    "TF * IDF\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 126)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "word_lem = WordNetLemmatizer()\n",
    "# Cleanin the text below are the steps we have to do as per requirement\n",
    "# removinng the stop words, stemming/lemmatization\n",
    "# lower case words\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [word_lem.lemmatize(w) for w in review if w not in stop_words]\n",
    "    review = \" \".join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "# creating the TF-IDF model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "cv = TfidfVectorizer()\n",
    "x = cv.fit_transform(corpus)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Both BOW and TF-IDF approach semantic information is not stored.\n",
    "TF-IDF gives importance to uncommon words\n",
    "There is definately chance of over fitting.\n",
    "\n",
    "# Word2Vec\n",
    "In this model, each model is basically represented as a vector of 32 or more dimension instead of single number\n",
    "\n",
    "Semantic information and relattion between different words is also preserved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('life', 0.2373637855052948),\n",
       " ('poverty', 0.22181925177574158),\n",
       " ('fortune', 0.21173998713493347),\n",
       " ('see', 0.20608000457286835),\n",
       " ('first', 0.19952735304832458),\n",
       " ('.', 0.17411324381828308),\n",
       " ('globally', 0.17079438269138336),\n",
       " ('milestones', 0.16289101541042328),\n",
       " ('british', 0.15985777974128723),\n",
       " ('four', 0.15461649000644684)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "# print(paragraph)\n",
    "# Preprocessing data\n",
    "review = re.sub(r'\\[[0-9]*\\]', ' ', paragraph)\n",
    "review = re.sub(r'\\s+', ' ', review)\n",
    "review = review.lower()\n",
    "review = re.sub(r'\\d', ' ', review)\n",
    "review = re.sub(r'\\s+', ' ', review)\n",
    "# print(review)\n",
    "\n",
    "# Preparing the dataset into sentences\n",
    "sentences = nltk.sent_tokenize(review)\n",
    "# print(sentences)\n",
    "# Convert into sentence into words\n",
    "sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# print(sentences)\n",
    "\n",
    "# Removing the stop words from the sentences \n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [w for w in sentences[i] if w not in stop_words]\n",
    "# print(sentences)\n",
    "\n",
    "# Training the Word2Vec model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "words = model.wv.vocab\n",
    "# print(words)\n",
    "# vector = model.wv['war']\n",
    "similar = model.wv.most_similar(\"freedom\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
