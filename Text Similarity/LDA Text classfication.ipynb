{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample documents\n",
    "doc_a = \"Problem with process Appserver LA01 process may be down\"\n",
    "doc_b = \"Problem with process Appserver LA02 process may be down\"\n",
    "doc_c = \"Problem with process Appserver LA03 process may be down\"\n",
    "doc_d = \"LAS1 CPU starvation detected possible high GC and low heap memory PEM_MAJOR\"\n",
    "doc_e = \"LAS2 CPU starvation detected possible high GC and low heap memory PEM_MAJOR.\" \n",
    "\n",
    "# compile sample documents into a list\n",
    "reviews_datasets = [doc_a, doc_b, doc_c, doc_d, doc_e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "doc_term_matrix = count_vect.fit_transform(reviews_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x13 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 29 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=5, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=42, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "LDA = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "LDA.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low\n",
      "pem_major\n",
      "problem\n",
      "process\n",
      "gc\n",
      "cpu\n",
      "memory\n",
      "possible\n",
      "problem\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for i in range(10):\n",
    "    random_id = random.randint(0,len(count_vect.get_feature_names()))\n",
    "    print(count_vect.get_feature_names()[random_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_vect.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  5,  6,  7,  8,  9, 12, 11,  0, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_topic_words = first_topic.argsort()[-10:]\n",
    "top_topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heap\n",
      "high\n",
      "low\n",
      "memory\n",
      "pem_major\n",
      "possible\n",
      "starvation\n",
      "process\n",
      "appserver\n",
      "problem\n"
     ]
    }
   ],
   "source": [
    "# These indexes can then be used to retrieve the value of the words from the count_vect object\n",
    "for i in top_topic_words:\n",
    "    print(count_vect.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['possible', 'starvation', 'process', 'appserver', 'problem']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['possible', 'starvation', 'process', 'appserver', 'problem']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['possible', 'starvation', 'process', 'appserver', 'problem']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['low', 'memory', 'pem_major', 'possible', 'starvation']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['possible', 'starvation', 'appserver', 'problem', 'process']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the 10 words with highest probabilities for all the five topics\n",
    "for i,topic in enumerate(LDA.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([count_vect.get_feature_names()[i] for i in topic.argsort()[-5:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rups/.virtualenvs/jupyiter_env/lib/python3.6/site-packages/sklearn/decomposition/_nmf.py:109: RuntimeWarning: invalid value encountered in sqrt\n",
      "  return np.sqrt(res * 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "    n_components=5, random_state=42, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using TFIDF factorization\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# create a document term matrix with TFIDF.\n",
    "tfidf_vect = TfidfVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "doc_term_matrix = tfidf_vect.fit_transform(reviews_datasets)\n",
    "\n",
    "#  create a probability matrix that contains probabilities of all the words in the vocabulary for all the topics\n",
    "nmf = NMF(n_components=5, random_state=42)\n",
    "nmf.fit(doc_term_matrix )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pem_major\n",
      "starvation\n",
      "heap\n",
      "detected\n",
      "memory\n",
      "pem_major\n",
      "detected\n",
      "starvation\n",
      "high\n",
      "low\n"
     ]
    }
   ],
   "source": [
    "# let's randomly get 10 words from our vocabulary\n",
    "import random\n",
    "\n",
    "for i in range(10):\n",
    "    random_id = random.randint(0,len(tfidf_vect.get_feature_names()))\n",
    "    print(tfidf_vect.get_feature_names()[random_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heap\n",
      "high\n",
      "low\n",
      "memory\n",
      "pem_major\n",
      "possible\n",
      "starvation\n",
      "process\n",
      "appserver\n",
      "problem\n"
     ]
    }
   ],
   "source": [
    "# indexes can now be passed to the tfidf_vect object to retrieve the actual words\n",
    "for i in top_topic_words:\n",
    "    print(tfidf_vect.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['heap', 'high', 'low', 'memory', 'pem_major', 'possible', 'starvation', 'appserver', 'problem', 'process']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['cpu', 'detected', 'gc', 'heap', 'high', 'low', 'memory', 'pem_major', 'possible', 'starvation']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['gc', 'heap', 'high', 'low', 'memory', 'pem_major', 'possible', 'problem', 'process', 'starvation']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['gc', 'heap', 'high', 'low', 'memory', 'pem_major', 'possible', 'problem', 'process', 'starvation']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['gc', 'heap', 'high', 'low', 'memory', 'pem_major', 'possible', 'problem', 'process', 'starvation']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the ten words with highest probabilities for each of the topics\n",
    "for i,topic in enumerate(nmf.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([tfidf_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-8772702ae337>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtopic_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_term_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreviews_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Topic'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mreviews_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "topic_values = nmf.transform(doc_term_matrix)\n",
    "reviews_datasets['Topic'] = topic_values.argmax(axis=1)\n",
    "reviews_datasets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference \n",
    "# https://stackabuse.com/python-for-nlp-topic-modeling/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
